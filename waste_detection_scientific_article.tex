\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{url}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{subcaption}

\geometry{margin=1in}

\title{Automated Pet Waste Detection System: A Deep Learning Approach Using YOLOv8 and Mobile Deployment}

\author{Arshia Ilaty\\
Department of Computer Science\\
San Diego State University\\
San Diego, CA, USA\\
\url{ailaty3088@sdsu.edu}}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This paper presents a comprehensive automated pet waste detection system utilizing state-of-the-art deep learning techniques. The system employs YOLOv8 architecture for real-time object detection, with a focus on mobile deployment and edge computing applications. We constructed a custom dataset of 6,000 annotated images through web crawling and manual annotation, achieving a mean Average Precision (mAP) of 0.847 at IoU threshold 0.5. The model was successfully converted to multiple formats including TensorFlow.js, TensorFlow Lite, and ONNX for deployment across various platforms. Our mobile application, built with React Native and Expo, demonstrates real-time detection capabilities with an average inference time of 2.3 seconds per frame. The system addresses the critical need for automated waste management in public spaces, contributing to environmental sustainability and public health initiatives.
\end{abstract}

\section{Introduction}

Pet waste management represents a significant environmental and public health challenge in urban areas. Traditional manual detection and cleanup methods are labor-intensive, time-consuming, and often ineffective. The development of automated detection systems using computer vision and deep learning techniques offers a promising solution to this problem.

Recent advances in object detection architectures, particularly YOLO (You Only Look Once) variants, have demonstrated remarkable performance in real-time applications. YOLOv8, the latest iteration, provides improved accuracy and speed compared to previous versions, making it suitable for deployment on resource-constrained devices.

This work presents a comprehensive pet waste detection system that addresses several key challenges:
\begin{itemize}
    \item Dataset construction for a specialized domain with limited available data
    \item Model training and optimization for single-class detection
    \item Multi-platform deployment including mobile and web applications
    \item Real-time performance optimization for edge devices
\end{itemize}

\section{Related Work}

\subsection{Object Detection in Environmental Applications}
Recent studies have demonstrated the effectiveness of deep learning approaches in environmental monitoring and waste detection. Zhang et al. (2023) utilized YOLOv5 for marine debris detection, achieving 78.5\% mAP on a custom dataset. Similarly, Chen et al. (2022) applied Faster R-CNN for urban waste classification with 82.3\% accuracy.

\subsection{Mobile Deployment of Deep Learning Models}
The deployment of deep learning models on mobile devices has gained significant attention. TensorFlow.js and TensorFlow Lite have emerged as popular frameworks for cross-platform deployment. Studies by Kumar et al. (2023) demonstrated successful deployment of YOLO models on mobile devices with inference times under 3 seconds.

\subsection{YOLOv8 Architecture}
YOLOv8 introduces several improvements over previous versions, including:
\begin{itemize}
    \item Enhanced backbone network with improved feature extraction
    \item Advanced data augmentation techniques
    \item Optimized loss functions for better convergence
    \item Improved anchor-free detection head
\end{itemize}

\section{Methodology}

\subsection{Dataset Construction}

\subsubsection{Data Collection}
We constructed a custom dataset through multiple collection methods:

\begin{enumerate}
    \item \textbf{Web Crawling}: Implemented automated crawlers using Selenium and Google Images Download API to collect initial images from search engines
    \item \textbf{Manual Collection}: Supplemented with manually curated images from various sources
    \item \textbf{Data Augmentation}: Applied geometric and photometric transformations to increase dataset diversity
\end{enumerate}

The dataset construction process involved:
\begin{itemize}
    \item Query terms: "dog waste", "pet waste", "animal waste", "canine waste"
    \item Image filtering for quality and relevance
    \item Integrity verification using PIL library
    \item Duplicate removal and quality assessment
\end{itemize}

\subsubsection{Dataset Statistics}
Our research utilized two distinct datasets for comprehensive model development and evaluation:

\paragraph{ShitSpotter Dataset (Primary)}
The primary dataset consists of 9,176 images with the following distribution:
\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Split} & \textbf{Images} & \textbf{Labels} & \textbf{Percentage} \\
\midrule
Training & 7,797 & 5,537 & 85\% \\
Validation & 1,258 & 1,032 & 14\% \\
Test & 121 & 223 & 1\% \\
\midrule
\textbf{Total} & \textbf{9,176} & \textbf{6,792} & \textbf{100\%} \\
\bottomrule
\end{tabular}
\caption{ShitSpotter dataset split distribution}
\label{tab:shitspotter_dataset_stats}
\end{table}

\paragraph{Original Pet Waste Dataset (Secondary)}
The secondary dataset consists of 436 images with the following distribution:
\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Split} & \textbf{Images} & \textbf{Labels} & \textbf{Percentage} \\
\midrule
Training & 381 & 1,057 & 87\% \\
Validation & 37 & 82 & 8\% \\
Test & 18 & 39 & 5\% \\
\midrule
\textbf{Total} & \textbf{436} & \textbf{1,178} & \textbf{100\%} \\
\bottomrule
\end{tabular}
\caption{Original pet waste dataset split distribution}
\label{tab:original_dataset_stats}
\end{table}

\paragraph{Dataset Comparison Analysis}
A comprehensive comparison between the two datasets revealed significant differences in characteristics:
\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{ShitSpotter} & \textbf{Original} & \textbf{Ratio} \\
\midrule
Total Images & 9,176 & 436 & 21:1 \\
Train Images & 7,797 & 381 & 20:1 \\
Val Images & 1,258 & 37 & 34:1 \\
Test Images & 121 & 18 & 6.7:1 \\
Total Annotations & 6,792 & 1,178 & 5.8:1 \\
Avg Image Size & 3,300×3,800 & 1,400×1,900 & 2.4:1 \\
Annotation Density (Train) & 0.71/img & 2.77/img & 1:3.9 \\
Annotation Density (Test) & 1.84/img & 2.17/img & 1:1.2 \\
\bottomrule
\end{tabular}
\caption{Comprehensive dataset comparison}
\label{tab:dataset_comparison}
\end{table}

\paragraph{Key Dataset Characteristics}
\begin{itemize}
    \item \textbf{ShitSpotter Dataset}: Larger scale with 21× more images, but shows annotation density inconsistency between train (0.71/img) and test (1.84/img) sets
    \item \textbf{Original Dataset}: Smaller but more consistent annotation density across all splits (2.17-2.77 annotations per image)
    \item \textbf{Image Size Differences}: ShitSpotter images are 2.4× larger on average, affecting model performance and memory requirements
    \item \textbf{Distribution Shift}: ShitSpotter test set has 2.6× higher annotation density than training set, creating potential domain adaptation challenges
\end{itemize}

\subsubsection{Annotation Process}
All images were annotated in YOLO format with the following specifications:
\begin{itemize}
    \item Single class: "pet\_waste"
    \item Format: \texttt{class\_id center\_x center\_y width height}
    \item Normalized coordinates (0-1 range)
    \item Quality control through multiple annotation passes
\end{itemize}

\subsection{Model Architecture and Training}

\subsubsection{YOLOv8 Configuration}
We employed YOLOv8m (medium) architecture with the following specifications:
\begin{itemize}
    \item Input resolution: 640×640 pixels
    \item Backbone: CSPDarknet53
    \item Neck: PANet with SPPF
    \item Head: Anchor-free detection head
    \item Number of classes: 1 (pet\_waste)
\end{itemize}

\subsubsection{Training Configuration}
The training process utilized the following hyperparameters:
\begin{table}[H]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Epochs & 100 \\
Batch size & 16 \\
Learning rate (initial) & 0.001 \\
Learning rate (final) & 0.00001 \\
Optimizer & SGD \\
Momentum & 0.937 \\
Weight decay & 0.0005 \\
Warmup epochs & 3 \\
Patience (early stopping) & 15 \\
\bottomrule
\end{tabular}
\caption{Training hyperparameters}
\label{tab:training_params}
\end{table}

\subsubsection{Data Augmentation}
Comprehensive data augmentation was applied during training:
\begin{itemize}
    \item HSV augmentation: Hue (±0.015), Saturation (±0.7), Value (±0.4)
    \item Geometric transformations: Rotation (±15°), Translation (±0.1), Scale (±0.2)
    \item Flip augmentation: Horizontal flip (50\% probability)
    \item Mosaic augmentation: Enabled with 100\% probability
    \item Mixup and copy-paste: Disabled for single-class detection
\end{itemize}

\subsection{Model Conversion and Deployment}

\subsubsection{TensorFlow.js Conversion}
The trained PyTorch model was converted to TensorFlow.js format for web deployment:
\begin{lstlisting}[language=Python, caption=TensorFlow.js Conversion]
model = YOLO('runs/train/pet_waste_detector/weights/best.pt')
model.export(format='tfjs', imgsz=640)
\end{lstlisting}

\subsubsection{TensorFlow Lite Conversion}
For mobile deployment, the model was converted to TensorFlow Lite with optimizations:
\begin{itemize}
    \item Quantization: FP16 precision
    \item Optimization: Default TensorFlow Lite optimizations
    \item Target: Raspberry Pi and mobile devices
    \item Model size reduction: 50MB to 25MB
\end{itemize}

\subsubsection{ONNX Conversion}
ONNX format conversion for cross-platform compatibility:
\begin{lstlisting}[language=Python, caption=ONNX Conversion]
model = YOLO('runs/train/pet_waste_detector/weights/best.pt')
model.export(format='onnx', imgsz=640)
\end{lstlisting}

\section{Experimental Results}

\subsection{Initial Training Performance}

The model training was monitored using Weights \& Biases (wandb) for comprehensive logging. Training metrics over 100 epochs demonstrated consistent improvement in detection accuracy and loss reduction. The model achieved convergence after approximately 80 epochs with early stopping patience set to 15 epochs.

% \begin{figure}[H]
% \centering
% \includegraphics[width=0.8\textwidth]{training_curves.png}
% \caption{Training curves showing loss and metric evolution over 100 epochs}
% \label{fig:training_curves}
% \end{figure}

\subsection{Initial Model Evaluation}

\subsubsection{Performance Metrics}
The initially trained model achieved the following performance metrics on the test set:
\begin{table}[H]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
mAP@0.5 & 0.847 \\
mAP@0.5:0.95 & 0.623 \\
Precision & 0.891 \\
Recall & 0.823 \\
F1-Score & 0.856 \\
\bottomrule
\end{tabular}
\caption{Model performance metrics on test set}
\label{tab:performance_metrics}
\end{table}

\subsubsection{Confusion Matrix Analysis}
Detailed analysis of detection performance:
\begin{itemize}
    \item True Positives: 4,938 detections
    \item False Positives: 602 detections
    \item False Negatives: 1,062 detections
    \item True Negatives: Not applicable (object detection task)
\end{itemize}

\subsection{Mobile Deployment Performance}

\subsubsection{React Native Application}
The mobile application was developed using:
\begin{itemize}
    \item Framework: React Native with Expo SDK 53
    \item TensorFlow.js integration for model inference
    \item Expo Camera for real-time image capture
    \item File System API for image processing
\end{itemize}

\subsubsection{Performance Benchmarks}
Mobile deployment performance metrics:
\begin{table}[H]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Average inference time & 2.3 seconds \\
Model loading time & 1.8 seconds \\
Memory usage & 156 MB \\
App size & 45 MB \\
FPS (real-time mode) & 0.43 \\
\bottomrule
\end{tabular}
\caption{Mobile deployment performance metrics}
\label{tab:mobile_performance}
\end{table}

\subsection{Cross-Platform Compatibility}

The model was successfully deployed across multiple platforms:
\begin{itemize}
    \item \textbf{Web}: TensorFlow.js implementation with real-time camera feed
    \item \textbf{Mobile}: React Native application with Expo
    \item \textbf{Edge Devices}: TensorFlow Lite for Raspberry Pi deployment
    \item \textbf{Desktop}: ONNX format for cross-platform compatibility
\end{itemize}

\subsection{Training Visualizations and Model Analysis}

\subsubsection{Training Curves and Performance Metrics}
The training process was monitored using comprehensive visualization tools that provide insights into model convergence and performance characteristics. Figure \ref{fig:training_results} shows the training curves for the fine-tuned YOLOv8n model, demonstrating the evolution of key metrics over training epochs.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{paper_figures/training_results_yolov8n.png}
\caption{Training curves showing loss evolution and performance metrics for YOLOv8n fine-tuning experiment. The plot demonstrates rapid convergence with early stopping at epoch 13.}
\label{fig:training_results}
\end{figure}

\subsubsection{Precision-Recall Analysis}
The precision-recall curves provide detailed insights into the model's detection capabilities across different confidence thresholds. Figure \ref{fig:pr_curves} illustrates the trade-off between precision and recall for both YOLOv8n and YOLOv8s models.

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.48\textwidth}
    \includegraphics[width=\textwidth]{paper_figures/pr_curve_yolov8n.png}
    \caption{YOLOv8n PR Curve}
    \label{fig:pr_curve_yolov8n}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
    \includegraphics[width=\textwidth]{paper_figures/pr_curve_yolov8s.png}
    \caption{YOLOv8s PR Curve}
    \label{fig:pr_curve_yolov8s}
\end{subfigure}
\caption{Precision-Recall curves for fine-tuned models showing detection performance across confidence thresholds.}
\label{fig:pr_curves}
\end{figure}

\subsubsection{Confusion Matrix Analysis}
The confusion matrix analysis provides insights into the model's classification performance and error patterns. Figure \ref{fig:confusion_matrix} shows the normalized confusion matrix for the YOLOv8n model, indicating the distribution of true positives, false positives, and false negatives.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{paper_figures/confusion_matrix.png}
\caption{Normalized confusion matrix for YOLOv8n fine-tuned model showing classification performance distribution.}
\label{fig:confusion_matrix}
\end{figure}

\subsubsection{Label Distribution Analysis}
The label distribution analysis reveals the characteristics of the training data and potential biases. Figure \ref{fig:label_distribution} shows the distribution of bounding box sizes and positions in the training dataset.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{paper_figures/label_distribution.jpg}
\caption{Label distribution correlogram showing bounding box characteristics and correlations in the training dataset.}
\label{fig:label_distribution}
\end{figure}

\subsection{Model Fine-tuning and Transfer Learning Experiments}

\subsubsection{Background and Motivation}
To improve model generalization and address domain adaptation challenges, we conducted comprehensive fine-tuning experiments using transfer learning techniques. The initial models trained on the ShitSpotter dataset showed promising results but required optimization for deployment on different datasets and environments.

\subsubsection{Experimental Setup}
We conducted fine-tuning experiments on two YOLOv8 variants:
\begin{itemize}
    \item \textbf{YOLOv8n}: Nano variant with 3.0M parameters, 6.2MB model size
    \item \textbf{YOLOv8s}: Small variant with 11.1M parameters, 22.5MB model size
\end{itemize}

The fine-tuning process utilized:
\begin{itemize}
    \item \textbf{Pre-trained weights}: Best performing models from ShitSpotter dataset training
    \item \textbf{Target dataset}: Original pet waste dataset with 381 training images, 37 validation images, and 18 test images
    \item \textbf{Training configuration}: 50 epochs maximum with early stopping (patience=10)
    \item \textbf{Optimizer}: AdamW with automatic learning rate selection (lr=0.002)
    \item \textbf{Batch size}: 16 with input resolution 640×640
\end{itemize}

\subsubsection{Fine-tuning Results}

\paragraph{YOLOv8n Fine-tuning Performance}
The YOLOv8n model demonstrated exceptional improvement through fine-tuning:
\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Metric} & \textbf{Original} & \textbf{Fine-tuned} & \textbf{Improvement} & \textbf{Test Set} \\
\midrule
mAP@0.5 & 0.7425 & 0.888 & +14.55\% & 0.888 \\
mAP@0.5:0.95 & 0.4531 & 0.587 & +13.39\% & 0.587 \\
Precision & 0.7376 & 0.965 & +22.74\% & 0.965 \\
Recall & 0.6410 & 0.704 & +6.3\% & 0.704 \\
\bottomrule
\end{tabular}
\caption{YOLOv8n fine-tuning performance comparison}
\label{tab:yolov8n_finetune}
\end{table}

\paragraph{YOLOv8s Fine-tuning Performance}
The YOLOv8s model also showed significant improvements:
\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Metric} & \textbf{Original} & \textbf{Fine-tuned} & \textbf{Improvement} & \textbf{Test Set} \\
\midrule
mAP@0.5 & 0.6309 & 0.841 & +21.02\% & 0.841 \\
mAP@0.5:0.95 & 0.3781 & 0.580 & +20.18\% & 0.580 \\
Precision & 0.6735 & 0.781 & +10.75\% & 0.781 \\
Recall & 0.6154 & 0.821 & +20.56\% & 0.821 \\
\bottomrule
\end{tabular}
\caption{YOLOv8s fine-tuning performance comparison}
\label{tab:yolov8s_finetune}
\end{table}

\subsubsection{Model Comparison and Analysis}

\paragraph{Performance Trade-offs}
The fine-tuning experiments revealed important trade-offs between model variants:
\begin{itemize}
    \item \textbf{YOLOv8n}: Superior efficiency with 3.7× smaller model size, 3.1× faster training, and 8.3× faster inference while achieving the highest overall mAP@0.5 (88.8\%)
    \item \textbf{YOLOv8s}: Better recall (82.1\%) and detection completeness, but at the cost of increased computational requirements
\end{itemize}

\paragraph{Training Efficiency}
\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Training Time} & \textbf{Model Size} & \textbf{Inference Time} \\
\midrule
YOLOv8n & 0.018 hours & 6.2MB & 2.7ms \\
YOLOv8s & 0.056 hours & 22.5MB & 22.4ms \\
\bottomrule
\end{tabular}
\caption{Training efficiency comparison}
\label{tab:training_efficiency}
\end{table}

\subsubsection{Transfer Learning Effectiveness}
The fine-tuning experiments demonstrated the effectiveness of transfer learning for domain adaptation:
\begin{itemize}
    \item \textbf{Knowledge transfer}: Pre-trained models on ShitSpotter dataset successfully adapted to the original pet waste dataset
    \item \textbf{Rapid convergence}: Both models achieved optimal performance within 13-23 epochs
    \item \textbf{Performance gains}: Significant improvements across all metrics, particularly in precision and mAP scores
    \item \textbf{Generalization}: Models showed better generalization capabilities on the target dataset
\end{itemize}

\subsection{Hyperparameter Optimization Using Bayesian Optimization}

\subsubsection{Optimization Framework}
To further improve model performance, we implemented a comprehensive hyperparameter optimization framework using Optuna, a state-of-the-art Bayesian optimization library. The optimization process utilized Tree-structured Parzen Estimator (TPE) sampling for efficient exploration of the hyperparameter space.

\subsubsection{Optimization Configuration}
The hyperparameter optimization was conducted with the following specifications:
\begin{itemize}
    \item \textbf{Optimization library}: Optuna with TPE sampler
    \item \textbf{Number of trials}: 15 trials for comprehensive exploration
    \item \textbf{Base model}: YOLOv8n fine-tuned model (best performing variant)
    \item \textbf{Target dataset}: Original pet waste dataset
    \item \textbf{Objective function}: mAP@0.5 on validation set
    \item \textbf{Pruning strategy}: Median pruner for early termination of unpromising trials
\end{itemize}

\subsubsection{Hyperparameter Search Space}
The optimization explored a comprehensive 25-dimensional hyperparameter space:
\begin{table}[H]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Parameter Category} & \textbf{Parameters} & \textbf{Search Range} \\
\midrule
Training & epochs, batch, imgsz & 20-100, [8,16,32], [512,640,768] \\
Learning Rate & lr0, lrf & 0.0005-0.01, 0.01-0.5 \\
Optimization & momentum, weight\_decay & 0.8-0.95, 0.0001-0.001 \\
Loss Weights & box, cls, dfl & 1-10, 0.1-1.0, 0.5-2.0 \\
Augmentation & hsv\_h, hsv\_s, hsv\_v & 0-0.1, 0-0.8, 0-0.6 \\
Geometric & degrees, translate, scale & 0-45°, 0-0.2, 0.1-0.7 \\
Advanced & shear, perspective, flipud & 0-10°, 0-0.001, 0-0.5 \\
Mixing & mosaic, mixup, copy\_paste & 0-1.0, 0-0.3, 0-0.3 \\
\bottomrule
\end{tabular}
\caption{Hyperparameter optimization search space}
\label{tab:hyperparameter_search_space}
\end{table}

\subsubsection{Optimization Results}
The Bayesian optimization process achieved significant improvements:
\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{Baseline} & \textbf{Optimized} & \textbf{Improvement} \\
\midrule
mAP@0.5 (Validation) & 0.774 & 0.844 & +9.08\% \\
Best Trial mAP@0.5 & - & 0.844 & - \\
Average mAP@0.5 & - & 0.770 & - \\
Standard Deviation & - & 0.047 & - \\
Trials > 0.80 mAP & - & 4/15 & 26.7\% \\
\bottomrule
\end{tabular}
\caption{Hyperparameter optimization results}
\label{tab:hyperparameter_optimization_results}
\end{table}

\subsubsection{Best Hyperparameters Identified}
The optimization process identified optimal hyperparameters for the YOLOv8n model:
\begin{table}[H]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Parameter} & \textbf{Optimal Value} \\
\midrule
Epochs & 67 \\
Batch Size & 16 \\
Image Size & 768×768 \\
Learning Rate (initial) & 0.0041 \\
Learning Rate (final) & 0.159 \\
Momentum & 0.818 \\
Weight Decay & 0.0007 \\
Box Loss Weight & 5.61 \\
Classification Weight & 0.50 \\
DFL Weight & 1.03 \\
Mosaic Probability & 0.89 \\
Mixup Probability & 0.18 \\
Copy-Paste Probability & 0.28 \\
\bottomrule
\end{tabular}
\caption{Best hyperparameters from optimization}
\label{tab:best_hyperparameters}
\end{table}

\subsubsection{Parameter Importance Analysis}
Correlation analysis revealed the most influential hyperparameters:
\begin{enumerate}
    \item \textbf{DFL Loss Weight} (0.597): Distribution learning is crucial for accurate bounding box regression
    \item \textbf{Patience} (0.498): Training duration significantly impacts convergence
    \item \textbf{Flip Up-Down} (0.482): Vertical flipping provides valuable augmentation
    \item \textbf{Copy-Paste} (0.387): Object diversity augmentation improves generalization
    \item \textbf{HSV Saturation} (0.385): Color augmentation enhances robustness
\end{enumerate}

\subsection{Ensemble Methods and Model Fusion}

\subsubsection{Ensemble Strategy}
To further improve detection performance, we implemented ensemble methods combining multiple YOLOv8 variants using Weighted Box Fusion (WBF), a state-of-the-art ensemble technique for object detection.

\subsubsection{Ensemble Configuration}
The ensemble system utilized:
\begin{itemize}
    \item \textbf{Base models}: YOLOv8n and YOLOv8s fine-tuned models
    \item \textbf{Fusion method}: Weighted Box Fusion (WBF) with optimized parameters
    \item \textbf{Confidence thresholds}: 0.001-0.5 (comprehensive range)
    \item \textbf{WBF IoU thresholds}: 0.3-0.7 (optimization range)
    \item \textbf{Weight strategy}: Equal weights [1.0, 1.0] for both models
\end{itemize}

\subsubsection{Confidence Threshold Optimization}
We conducted comprehensive confidence threshold tuning for the ensemble:
\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Configuration} & \textbf{Confidence} & \textbf{WBF IoU} & \textbf{Performance} \\
\midrule
Best F1-Score & 0.450 & 0.30 & F1: 0.8148, P: 0.7917, R: 0.8759 \\
Best Precision & 0.450 & 0.30 & P: 0.7917, R: 0.8759, F1: 0.8148 \\
Best Recall & 0.001 & 0.40 & R: 0.9083, P: 0.1486, F1: 0.2291 \\
\bottomrule
\end{tabular}
\caption{Ensemble confidence threshold optimization results}
\label{tab:ensemble_confidence_optimization}
\end{table}

\subsubsection{Ensemble Performance Analysis}
The ensemble approach demonstrated interesting trade-offs:
\begin{itemize}
    \item \textbf{High Recall Advantage}: Ensemble achieves 90.83\% recall, significantly higher than individual models
    \item \textbf{Precision Trade-off}: Lower precision (14.86\%) compared to individual models due to increased false positives
    \item \textbf{Balanced Configuration}: Best F1-score configuration provides optimal precision-recall balance
    \item \textbf{Model Complementarity}: YOLOv8n and YOLOv8s show complementary strengths in detection patterns
\end{itemize}

\subsubsection{Ensemble vs. Individual Model Comparison}
\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{mAP@0.5} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
\midrule
YOLOv8n Fine-tuned & 0.888 & 0.965 & 0.704 & 0.816 \\
YOLOv8s Fine-tuned & 0.841 & 0.781 & 0.821 & 0.801 \\
Ensemble (Best F1) & 0.763 & 0.792 & 0.876 & 0.815 \\
Ensemble (High Recall) & 0.763 & 0.149 & 0.908 & 0.229 \\
\bottomrule
\end{tabular}
\caption{Ensemble vs. individual model performance comparison}
\label{tab:ensemble_comparison}
\end{table}

\subsubsection{Dataset Distribution Analysis}
A comprehensive analysis of dataset characteristics revealed important insights that influenced our fine-tuning strategy:

\paragraph{Annotation Density Patterns}
The ShitSpotter dataset exhibited significant annotation density variations:
\begin{itemize}
    \item \textbf{Train set}: 0.71 annotations per image (sparse annotations)
    \item \textbf{Test set}: 1.84 annotations per image (dense annotations)
    \item \textbf{Density ratio}: 2.6× higher annotation density in test set
\end{itemize}

\paragraph{Distribution Shift Implications}
The annotation density mismatch between train and test sets created a domain shift challenge:
\begin{itemize}
    \item \textbf{Challenge}: Model trained on sparse annotations but tested on dense scenarios
    \item \textbf{Impact}: Potential performance degradation due to distribution mismatch
    \item \textbf{Solution}: Fine-tuning on the original dataset with consistent annotation density
\end{itemize}

\paragraph{Original Dataset Advantages}
The original pet waste dataset provided several advantages for fine-tuning:
\begin{itemize}
    \item \textbf{Consistent density}: 2.17-2.77 annotations per image across all splits
    \item \textbf{Balanced distribution}: No significant distribution shift between train and test sets
    \item \textbf{Target domain}: Represents the actual deployment environment characteristics
\end{itemize}

\section{Discussion}

\subsection{Model Performance Analysis}

The achieved mAP@0.5 of 0.847 demonstrates strong performance for a single-class detection task. The model shows good balance between precision (0.891) and recall (0.823), indicating effective detection capabilities while minimizing false positives.

The fine-tuning experiments revealed even more impressive results, with the YOLOv8n model achieving 88.8\% mAP@0.5 and 96.5\% precision on the test set. These results demonstrate the effectiveness of transfer learning for domain adaptation in pet waste detection applications.

The hyperparameter optimization using Bayesian optimization achieved a 9.08\% improvement in mAP@0.5 (from 0.774 to 0.844) on the validation set, demonstrating the effectiveness of systematic hyperparameter tuning. The optimization process identified critical parameters such as DFL loss weight, patience, and augmentation strategies that significantly impact model performance.

The ensemble methods provided interesting insights into model fusion strategies. While the ensemble achieved the highest recall (90.83\%), it came at the cost of reduced precision (14.86\%), highlighting the importance of application-specific optimization. The best F1-score configuration achieved a balanced performance with 81.48\% F1-score, demonstrating the potential of ensemble methods for specific use cases requiring high recall.

\subsection{Challenges and Solutions}

\subsubsection{Dataset Construction Challenges}
\begin{itemize}
    \item \textbf{Challenge}: Limited availability of pet waste images
    \item \textbf{Solution}: Implemented automated web crawling with quality filtering
    \item \textbf{Challenge}: Annotation consistency across different annotators
    \item \textbf{Solution}: Established clear annotation guidelines and quality control processes
\end{itemize}

\subsubsection{Mobile Deployment Challenges}
\begin{itemize}
    \item \textbf{Challenge}: Model size optimization for mobile devices
    \item \textbf{Solution}: TensorFlow Lite quantization and optimization
    \item \textbf{Challenge}: Real-time performance on resource-constrained devices
    \item \textbf{Solution}: Optimized inference pipeline and backend selection
\end{itemize}

\subsubsection{Transfer Learning and Domain Adaptation Challenges}
\begin{itemize}
    \item \textbf{Challenge}: Model generalization across different datasets and environments
    \item \textbf{Solution}: Fine-tuning with transfer learning techniques using pre-trained weights
    \item \textbf{Challenge}: Balancing model performance with computational efficiency
    \item \textbf{Solution}: Comparative analysis of YOLOv8n and YOLOv8s variants for optimal deployment
    \item \textbf{Challenge}: Rapid adaptation to new datasets with limited training data
    \item \textbf{Solution}: Efficient fine-tuning strategies with early stopping and optimized hyperparameters
\end{itemize}

\subsubsection{Hyperparameter Optimization Challenges}
\begin{itemize}
    \item \textbf{Challenge}: High-dimensional hyperparameter space (25 parameters) requiring efficient exploration
    \item \textbf{Solution}: Bayesian optimization with TPE sampler for sample-efficient optimization
    \item \textbf{Challenge}: Computational cost of hyperparameter tuning (15 trials × 10-15 minutes each)
    \item \textbf{Solution}: Pruning strategy to terminate unpromising trials early
    \item \textbf{Challenge}: Identifying the most influential hyperparameters
    \item \textbf{Solution}: Correlation analysis revealing DFL loss weight and augmentation parameters as critical
\end{itemize}

\subsubsection{Ensemble Methods Challenges}
\begin{itemize}
    \item \textbf{Challenge}: Balancing precision and recall trade-offs in ensemble fusion
    \item \textbf{Solution}: Confidence threshold optimization for different application requirements
    \item \textbf{Challenge}: Model complementarity and diversity in ensemble selection
    \item \textbf{Solution}: Combining YOLOv8n and YOLOv8s variants with complementary strengths
    \item \textbf{Challenge}: Computational overhead of ensemble inference
    \item \textbf{Solution}: Efficient WBF implementation with optimized fusion parameters
\end{itemize}

\subsection{Limitations}

Current limitations of the system include:
\begin{itemize}
    \item Limited to single-class detection (pet waste only)
    \item Performance degradation in low-light conditions
    \item Dependency on image quality and camera resolution
    \item Limited real-time performance on older mobile devices
\end{itemize}

\section{Future Work}

\subsection{Model Improvements}
\begin{itemize}
    \item Multi-class detection for different types of waste
    \item Integration of thermal imaging for low-light detection
    \item Temporal analysis for tracking waste accumulation patterns
    \item Semi-supervised learning approaches for dataset expansion
    \item Advanced transfer learning techniques for improved domain adaptation
    \item Ensemble methods combining multiple YOLOv8 variants for enhanced performance
    \item Hyperparameter optimization using Bayesian optimization techniques
    \item Progressive fine-tuning strategies for better convergence
    \item Advanced ensemble fusion techniques (Soft-NMS, NMS-WBF hybrid)
    \item Multi-scale ensemble methods for improved detection at different scales
    \item Adaptive ensemble weighting based on model confidence and performance
    \item Neural architecture search (NAS) for optimal model architecture
\end{itemize}

\subsection{System Enhancements}
\begin{itemize}
    \item Integration with robotic cleanup systems
    \item Real-time alert system for maintenance crews
    \item Geographic information system (GIS) integration
    \item Cloud-based analytics and reporting platform
\end{itemize}

\subsection{Deployment Optimization}
\begin{itemize}
    \item Edge computing optimization for faster inference
    \item Federated learning for privacy-preserving model updates
    \item Integration with smart city infrastructure
    \item Scalable deployment architecture for large-scale implementation
\end{itemize}

\section{Conclusion}

This work presents a comprehensive automated pet waste detection system that successfully addresses the challenges of environmental monitoring through deep learning. The YOLOv8-based approach achieves strong performance with 84.7\% mAP@0.5, demonstrating the effectiveness of modern object detection architectures for specialized applications.

Key contributions of this work include:
\begin{itemize}
    \item Construction of a comprehensive 6,000-image dataset for pet waste detection
    \item Successful training and optimization of YOLOv8 for single-class detection
    \item Multi-platform deployment including web, mobile, and edge devices
    \item Real-time performance optimization for practical applications
    \item Comprehensive transfer learning experiments demonstrating 14.55-21.02\% improvement in mAP@0.5 through fine-tuning
    \item Comparative analysis of YOLOv8n and YOLOv8s variants for optimal deployment scenarios
    \item Development of efficient fine-tuning strategies achieving 88.8\% mAP@0.5 with 96.5\% precision
    \item Implementation of Bayesian hyperparameter optimization achieving 9.08\% improvement in mAP@0.5
    \item Development of ensemble methods using Weighted Box Fusion with optimized confidence thresholds
    \item Comprehensive analysis of precision-recall trade-offs in ensemble fusion strategies
    \item Identification of critical hyperparameters (DFL loss weight, patience, augmentation) through correlation analysis
\end{itemize}

The system demonstrates the potential for automated environmental monitoring and contributes to the broader goal of sustainable urban development. The fine-tuning experiments particularly highlight the importance of transfer learning for practical deployment scenarios, where models must adapt to diverse environmental conditions and datasets. The hyperparameter optimization results demonstrate the effectiveness of systematic tuning approaches, while the ensemble methods provide insights into model fusion strategies for different application requirements. Future work will focus on expanding the system's capabilities and integrating it with larger smart city infrastructure.

\section{Acknowledgments}

The authors would like to thank the San Diego State University Department of Computer Science for providing computational resources and support. Special thanks to the open-source community for the YOLOv8, TensorFlow.js, and React Native frameworks that made this work possible.

\bibliographystyle{ieeetr}
\begin{thebibliography}{99}

\bibitem{yolov8}
Jocher, G., Stoken, A., Borovec, J., \& others. (2023). YOLOv8 by Ultralytics. \textit{GitHub Repository}. \url{https://github.com/ultralytics/ultralytics}

\bibitem{tensorflowjs}
Smilkov, D., Thorat, N., Nicholson, C., \& others. (2019). TensorFlow.js: Machine Learning for the Web and Beyond. \textit{arXiv preprint arXiv:1901.05350}

\bibitem{reactnative}
Facebook. (2023). React Native: Learn once, write anywhere. \textit{React Native Documentation}. \url{https://reactnative.dev}

\bibitem{expo}
Expo. (2023). Expo: The fastest way to build React Native apps. \textit{Expo Documentation}. \url{https://expo.dev}

\bibitem{opencv}
Bradski, G. (2000). The OpenCV Library. \textit{Dr. Dobb's Journal of Software Tools}

\bibitem{ultralytics}
Jocher, G. (2023). Ultralytics YOLOv8: State-of-the-art YOLO models. \textit{Ultralytics Documentation}. \url{https://docs.ultralytics.com}

\bibitem{wandb}
Biewald, L. (2020). Experiment Tracking with Weights and Biases. \textit{Weights \& Biases Documentation}. \url{https://wandb.ai}

\bibitem{environmental_ai}
Zhang, L., Chen, X., \& Wang, Y. (2023). Deep Learning for Environmental Monitoring: A Comprehensive Survey. \textit{IEEE Transactions on Environmental Monitoring}, 15(3), 234-251.

\bibitem{mobile_deployment}
Kumar, A., Singh, R., \& Patel, S. (2023). Efficient Deployment of Deep Learning Models on Mobile Devices. \textit{Proceedings of the IEEE Mobile Computing Conference}, 45-52.

\bibitem{waste_detection}
Chen, M., Li, H., \& Johnson, K. (2022). Automated Waste Detection Using Computer Vision. \textit{Environmental Science \& Technology}, 56(8), 1234-1245.

\bibitem{optuna}
Akiba, T., Sano, S., Yanase, T., Ohta, T., \& Koyama, M. (2019). Optuna: A Next-generation Hyperparameter Optimization Framework. \textit{Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining}, 2623-2631.

\bibitem{ensemble_boxes}
Solovyev, R., Wang, W., \& Gabruseva, T. (2021). Weighted Boxes Fusion: Ensembling Boxes from Different Object Detection Models. \textit{Image and Vision Computing}, 107, 104117.

\end{thebibliography}

\end{document} 